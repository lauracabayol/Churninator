\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
% ref packages
\usepackage{nameref}
% folowing  must be in this order
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{listings}

\usepackage[utf8]{inputenc} 
\usepackage[a4paper,
            left=2cm,
            right=2cm,
            top=2cm,
            bottom=2cm]{geometry}

\usepackage[english]{babel}
\usepackage[skip=10pt plus2pt, indent=10pt]{parskip}
\begin{document}

\section{Introduction}
Churn prediction is a crucial task in customer relationship management, aiming to identify customers who are likely to discontinue using a service. Accurately predicting churn allows companies to take proactive measures to retain these customers. In this task, we address bank churn from costumers data using machine-learning techniques. 

\section{Data description and preprocessing}
The dataset contains a total of 10,000 samples, where each sample represents a unique customer. Each sample contains both structured and unstructured data. The structured data includes features such as the number of products owned by the customer, their account balance, salary, age, gender, and other information. The dataset exhibits a significant class imbalance, which can affect the performance of machine-learning models. To illustrate the class imbalance, the left panel on Fig.\ref{fig:class_distribution} shows the distribution of costumers leaving (1) and not leaving (0) the service. 

To prepare the data for analysis, we first encode the categorical features (gender and country) into numerical labels. The encoding process transforms each unique category into an integer label. This step is essential because classical machine-learning algorithms require numerical input. 

The next steps involve handling the unstructured data and integrating it with the structured data. Customer feedback is a valuable source of unstructured data that provides insights into customer satisfaction and potential churn risk.  We perform sentiment analysis to classify the feedback as very positive, positive, neutral, negative, and very negative employing the BERT (Bidirectional Encoder Representations from Transformers) model. The first step is tokenization with the \texttt{BertTokenizer}. It is used to convert the text of each feedback into a format suitable for BERT. This involves breaking the text into tokens and mapping each token to an integer index in the BERT vocabulary. The tokenized text is then passed to the \texttt{BertForSequenceClassification module}. This module is fine-tuned to classify each piece of feedback into one of five categories, corresponding to star ratings ranging from 1 star (the poorest score) to 5 stars (the best score).

Once each feedback has been classified into a star rating, we encode these ratings into integer labels. We also include in the analysis a derived variable from the sentiment analysis that indicates how certain BERT is in the classification of costumer's feedback. This variable can help mitigate the impact of errors from BERT's predictions.

Once the data is all preprocessed, with a format that algorithms can read, we can look at the correlation between different variables and the probability of customer churn. The right panel on Fig.~\ref{fig:class_distribution} shows the correlation between all features in the dataset and their current status (Exited/not Exited). It clearly indicates that customer feedback is an important feature in predicting churn. However, we also found instances where customers gave excellent ratings accordiant to \texttt{BERT} (5 stars) but still left the service. Manual verification confirmed the high ratings, suggesting that despite positive feedback, these customers decided to churn. Conversely, some customers gave poor ratings (1 or 2 stars) but did not leave the service. These findings highlight the complexity of human behavior. While customer feedback is a relevant feature, it is not an infallible predictor of churn. Unexpectedly, the second most correlated feature with churn is age. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/class_distribution.png}
    \includegraphics[width=0.45\textwidth]{figures/correlation_matrix.png}
    \caption{\emph{Left:} Class distribution of the dataset (O:Non exited, 1:Exited). \emph{Right:} Correlation between the features in the dataset and the exited outcome}
    \label{fig:class_distribution}
\end{figure}

\section{Main results}
To address the churn prediction task, we employed two machine learning approaches: Gradient Boosting and a Neural Network. Other algorithms were also tested, but Gradient Boosting provided the best results. Both algorithms were optimized using \texttt{Optuna}, a hyperparameter optimization framework. \texttt{Optuna} allows for efficient exploration of the hyperparameter space to find the best performing settings for each model.

The performance of the models was evaluated using two primary metrics:
\begin{itemize}
    \item \textbf{ROC Curve:} The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate (sensitivity) versus the false positive rate (1-specificity) at various threshold settings. The area under the ROC curve (AUC) provides a single metric to evaluate the overall performance of the model, with higher values indicating better performance.
    \item \textbf{Confusion Matrix:} The confusion matrix provides a detailed breakdown of a model's performance by displaying the counts of true positives, true negatives, false positives, and false negatives. This matrix evaluates how well the model performs on each individual class. In the context of imbalanced datasets, understanding these metrics becomes critical since model might achieve a high overall accuracy by predominantly predicting the majority class while neglecting the minority class. 
\end{itemize}

\subsection{Gradient Boosting results}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/roc_curve_gb.png}
    \includegraphics[width=0.45\textwidth]{figures/confusion_matrix_gb.png}
    \caption{ROC Curve for Gradient Boosting}
    \label{fig:roc_curve_gb}
\end{figure}

The Gradient Boosting model achieved an AUC of 0.91 (left panel on Fig.\ref{fig:roc_curve_gb}). The confusion matrix (right panel on Fig.\ref{fig:roc_curve_gb}) indicated the following performance:
\begin{itemize}
    \item \textbf{True Negatives:} 95\% of the customers who did not leave the service were correctly predicted.
    \item \textbf{True Positives:} 70\% of the customers who left the service were correctly predicted.
\end{itemize}

\subsection{Neural Network results}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/roc_curve_nn.png}
    \includegraphics[width=0.45\textwidth]{figures/confusion_matrix_nn.png}
    \caption{ROC Curve for Neural Network}
    \label{fig:roc_curve_nn}
\end{figure}

On the other hand, the Neural Network model yielded slightly worse results:
\begin{itemize}
    \item \textbf{AUC:} Approximately 0.86 (left panel on Fig.\ref{fig:roc_curve_nn}).
    \item \textbf{True Negatives:} 90\% of the customers who did not leave the service were correctly predicted (right panel on Fig.\ref{fig:roc_curve_nn})
    \item \textbf{True Positives:} 70\% of the customers who left the service were correctly predicted.
\end{itemize}

The slightly lower performance of the Neural Network could be attributed to its sensitivity to dummy variables and the need for data normalization. Although normalization was performed, further optimization might improve results.

\subsection{Discussion}

Despite the overall performance of the models, churn prediction is not perfect. One significant challenge is the inherent unpredictability of human behavior. As previously noted, some customers with excellent feedback ratings still chose to leave the service, while others with poor ratings decided to stay. This unpredictability introduces complexity that machine learning models may not fully capture.

Another factor contributing to the imperfection is the class imbalance in the dataset. Although methods like under-sampling, over-sampling, and bias adjustment for the neural network were applied to mitigate this issue, these techniques did not lead to a substantial improvement in the model's performance. 

To enhance the accuracy and robustness of churn prediction, several avenues for future work are suggested:
\begin{itemize}
    \item \textbf{Ensemble Methods:} Exploring ensemble methods that combine predictions from multiple models could potentially improve performance (bagging, boosting, and stacking).
    \item \textbf{Advanced Imbalance Handling:} Further investigation into advanced techniques for handling imbalanced data is warranted. This could include advanced cost-sensitive learning approaches.
\end{itemize}

\end{document}